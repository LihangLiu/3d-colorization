-- pure coor cube --

epoch 11:
	pure color cube

epoch 12:
	11 + random noise in x

-- noise_x --

epoch 13:
	gan + noise in x

epoch 14:
	13 + noise stddev=.1

epoch 15:
	14 + smooth label=0.75
	tbd

epoch 16:
	cgan: new G by conv+deconv
	      input  vox_a	

epoch 17:
	16 + noise z

epoch 18:
	cgan: G by decoder+encoder
	      input vox_a,z

-- fatal bug found on noise_x --
-- epoch 13-18 invalid	      --

epoch 19:
	gan + noise_x(dev=.1)

epoch 20:
	cgan: G by conv+deconv
              input vox_a,z

epoch 21:
	cgan: G by conv+deconv
	      input vox_a

-- change G/D train ratio --
-- 5 -> 1 --

epoch 22:
	19: ratio=1

epoch 23:
	20: ratio=1

epoch 24:
	23: cgan+unet
	crash after 1000 iters	

epoch 25:
	23: cgan+aloss
	crash after 1700 iters

-- change G/D train ratio --
-- 1 -> 5 --

epoch 26:
	24: cgan+unet, ratio=5

epoch 27:
	23: cgan+aloss, ratio=5

epoch 28:
	26+27: cgan+unet+aloss, ratio=5

-- add mask --

epoch 29: ***
	20: cgan+mask
	a->G->rgb, (a+rgb)->D->

epoch 30:
	29: cgan+unet+mask
	crash: loss don't change

-- deeper G --

epoch 31:
	29: cgan+mask, deeperG
	    ngf = 16
	crash: loss don't change

epoch 32: ***
	31: cgan+mask, deeperG
	    ngf = 8

epoch 33:
	32: cgan+mask, deeperG
	    h5: (n,1,1,1,f*8->f*16)

epoch 34:
	29: cgan+mask
	    symmetrize conv and deconv

-- vox dataset jitter	    --
-- shift, flip, rotate axis --

epoch 35:
	32: cgan+mask, deeperG
	    vox jitter(shift,flip,rotate axis)
	crash: loss don't change

epoch 36:
	35: cgan+mask, deeperG
            vox jitter(shift)
	crash: loss don't change

-- disable vox jitter --
-- unet on deeper G   --

epoch 37:
	32: cgan+unet+mask, deeperG
	    ngf = 8

epoch 38:
        37: cgan+unet+mask, deeperG
            ngf = 5
	no apparent improve using ngf=5

-- multi-resolution loss --

epoch 39: ****
	32: cgan+mask, deeperG
	    D: rgba->maxpooling->rgba_16->h1
	seems better than epoch 32

-- cwgan    --
-- noise_x --

epoch 40:
	32: cwgan
	    a. minibatch discriminator disabled
	    b. gan loss changed
	    c. clip on D
  	    d. more training on D, slower
	    noise_x noise stddev=1

epoch 41:
	40: cwgan
	    noise_x noise stddev=.1

epoch 42:
        40: cwgan
            remove noise_x

-- multi-resolution loss --

epoch 43:
	42: cwgan
	    mrl: D + rgba_16

epoch 44:
	43: cwgan
	    mrl: D + rgba_16
	    noise_x noise stddev=1

-- patch net & mrl --

epoch 45:
	44: cwgan
	    mrl: D + rgba_16
	    patch net

epoch 46:
	45: cwgan
	    mrl: D + rgba_16
	    patch net: reduce filter size:4->2

epoch 47:
	45: cwgan
	    mrl: D + rgba_16 & rgba_16 & rgba_8 & rgba_4
            patch net

-- sparse conv --

epoch 48:
	40: cwgan
	    sparse conv: scale=0.1

epoch 49:
        48: cwgan
            sparse conv: scale=0.1/numofparams

epoch 50:
	49: cwgan
            sparse conv: scale=10/numofparams

-- vox loss --

epoch 51:
	40: cwgan + rgbloss

-- conv 2.5d --

epoch 52:
	40: cwgan + conv2.5d
	    D=0.02, alpha=4 	# bug in 0.2 because wgan clip all D weights to be (-0.01,0.01)
	    D: h1-h3, z project

epoch 53:
	52: cwgan + conv2.5d
	    D: h1-h3, xyz project

epoch 54:
	52: cwgan + conv2.5d
	    D=0.02, alpha=4
            D: h1-h3, xyz project
	    G: h1-h3, xyz project

epoch 55:
	54: cwgan + conv2.5d
	    D=0.02, alpha=4
	    D: h1-h3, z project
	    G: h1-h3, z project

epoch 56:
	54: cwgan + conv2.5d
	    D=0.02, alpha=8
	    D: h1-h4, xyz project
	    G: h1-h4, xyz project

epoch 57:
	54: cwgan + conv2.5d
            D=0.02, alpha=8
            D: h1-h3, xyz project
            G: h1-h3, xyz project




